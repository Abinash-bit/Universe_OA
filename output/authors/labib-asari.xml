<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts by Labib Asari)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/authors/labib-asari.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 21 Aug 2023 00:53:03 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Moving towards OpenCL</title><link>http://openastronomy.org/Universe_OA/posts/2023/07/20230728_0000_labeeb-7z/</link><dc:creator>Labib Asari</dc:creator><description>&lt;h3 id="background"&gt;Background&lt;/h3&gt;

&lt;p&gt;So far, all my work on GPUs has been using CUDA. But CUDA is proprietary to NVIDIA and only works on NVIDIA GPUs. So, I’ve been working on moving the code to OpenCL, which is an open standard for parallel programming on heterogeneous systems.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;h3 id="opencl"&gt;OpenCL&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://www.khronos.org/opencl/"&gt;OpenCL&lt;/a&gt;(Open Computing Language) is an open standard for cross-platform, parallel programming of diverse accelerators(CPUs, GPUs, FPGAs, etc) found in supercomputers, cloud servers, personal computers, mobile devices and embedded platforms.
Note the 2 key points -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class="language-plaintext highlighter-rouge"&gt;open standard&lt;/code&gt; : this means that the specification and documentation of the technology are publicly available and can be accessed by anyone.&lt;/li&gt;
&lt;li&gt;&lt;code class="language-plaintext highlighter-rouge"&gt;cross-platform&lt;/code&gt; : this means that it can run on multiple operating systems and hardware architectures without requiring major modifications to the code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This makes OpenCL a very attractive option for developers who want to write code that can run on a wide range of devices. From Gnuastro’s perspective, this means that we can write code that can run on multiple GPU manufactureres, as well as CPUs and other accelerators. Our GPU kernels will be portable to any system, regardless of its configuration!&lt;/p&gt;

&lt;p&gt;Next point to consider is OpenCL is a &lt;code class="language-plaintext highlighter-rouge"&gt;standard&lt;/code&gt;. It is different from CUDA in this regard. CUDA is a framework, whereas OpenCL is a standard. What does this mean?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The OpenCL standard refers to the specification and guidelines set forth by the Khronos Group which is responsible for developing and maintaining the standard. The OpenCL standard defines the API, data types, functions, and programming model that developers must follow when writing code for OpenCL. It is a formal document that ensures uniformity and compatibility across different OpenCL implementations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OpenCL is not an open-source library! It basically defines how the library should behave(big simplification!).&lt;/p&gt;

&lt;p&gt;So what can we do with the standard alone? Not much! We need an &lt;code class="language-plaintext highlighter-rouge"&gt;implementation of the standard&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This also reminds me of the question I once had - What do you need to create a new programming language?
My first guess was a compiler! My thought process was if a program(compiler in this case) can understand my High level language and convert it to corresponding machine code, then I can write programs in that high level language for any task!
So all I’d need is a compiler for that language.
Its close, but not totally accurate.&lt;/p&gt;

&lt;p&gt;You dont actually need a compiler for a new programming language. You ONLY need a &lt;code class="language-plaintext highlighter-rouge"&gt;specification&lt;/code&gt; for it. The specification will define the syntax and semantics(rules) of the language.
You only need a compiler when you want to run programs using your language!(what good is a language if you cant run programs using it? haha)&lt;/p&gt;

&lt;p&gt;Similaraly OpenCL defines a set of rules which specify how it will behave. But to use OpenCL we need an implementation of this standard.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenCL implementations are software packages developed by hardware manufactureres that provide the necessary drivers and runtime libraries for running OpenCL applications on their specific hardware. Each hardware vendor is responsible for creating their own OpenCL implementation that conforms to the OpenCL standard. This means that each implementation may have its own unique features and quirks, but they all adhere to the same standard.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many different implementations available for it! (find the full list &lt;a href="https://www.khronos.org/conformance/adopters/conformant-products/opencl"&gt;here&lt;/a&gt; or &lt;a href="https://www.iwocl.org/resources/opencl-implementations/"&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Basically each of the hardware manfacturers provide an implementation of the OpenCL standard for their hardware. This implementation is usually provided as a framework. Depending on what hardware you have on your system, you can choose the corresponding framework to use.&lt;/p&gt;

&lt;h3 id="how-does-opencl-work"&gt;How does OpenCL work?&lt;/h3&gt;

&lt;p&gt;Here’s waht a typical OpenCL system looks like :&lt;/p&gt;

&lt;p&gt;&lt;img alt="opencl-sytem" src="https://labeeb-7z.github.io/Blogs/img/posts/opencl/opencl-system.png"&gt;&lt;/p&gt;

&lt;p&gt;OpenCL programs consist of two parts: host code and device code. The host code is written in C or C++ and runs on the host, while the device code is written in OpenCL C and runs on the device. The host code is responsible for setting up the OpenCL environment, creating the context, compiling the device code, and executing the kernels on the device.&lt;/p&gt;

&lt;p&gt;The device code is compiled at runtime by the host code. This means that the host code must be compiled first, and then the device code can be compiled. The host code is compiled using a standard C/C++ compiler, while the device code is compiled using the OpenCL compiler. The OpenCL compiler is provided by the OpenCL implementation and is responsible for compiling the device code into binary code that can be executed on the device.&lt;/p&gt;

&lt;p&gt;How does the OpenCL library interact with the hardware? Its made possible through OpenCL-ICD.&lt;/p&gt;

&lt;p&gt;OpenCL ICD stands for OpenCL Installable Client Driver. It is a component of the OpenCL&lt;/p&gt;

&lt;p&gt;It enables multiple manufacturers OpenCL drivers to coexist on a single system. Instead of having a single monolithic OpenCL driver, an ICD allows different manufactureres (e.g., NVIDIA, AMD, Intel) to provide their own separate OpenCL implementation as dynamically loadable libraries. This means that developers can select the appropriate OpenCL driver at runtime without needing to modify their applications.&lt;/p&gt;

&lt;p&gt;The ICD mechanism is crucial for achieving portability and flexibility in developing applications using computational power of various devices from different manufacturers.&lt;/p&gt;

&lt;h3 id="opencl-programming-model"&gt;OpenCL Programming Model&lt;/h3&gt;

&lt;p&gt;The Programming Model of OpenCL is very similar to CUDA which I covered in my previous post. However CUDA has a lot of abstraction since it has its own runtime library which communicates with the driver.
In OpenCL there’s direct communication with the drivers and the host code is responsible for setting up the environment so its a bit more lower level than CUDA.&lt;/p&gt;

&lt;p&gt;Some of the key terms in OpenCL are :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Work Item: Basic unit of work on a compute device&lt;/li&gt;
&lt;li&gt;Kernel: The code that runs on a work item (Basically a C function)&lt;/li&gt;
&lt;li&gt;Program: Collection of kernels and other functions&lt;/li&gt;
&lt;li&gt;Context: The environment where work-items execute (Devices, their memories and command queues)&lt;/li&gt;
&lt;li&gt;Command Queue: Queue used by the host to submit work (kernels, memory copies) to the device.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’ll cover the programming aspect of OpenCL in more detail in my next post.&lt;/p&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2023/07/20230728_0000_labeeb-7z/</guid><pubDate>Thu, 27 Jul 2023 23:00:00 GMT</pubDate></item><item><title>GPUs and Convolutions in Gnuastro</title><link>http://openastronomy.org/Universe_OA/posts/2023/07/20230704_0000_labeeb-7z/</link><dc:creator>Labib Asari</dc:creator><description>&lt;h3 id="background"&gt;Background&lt;/h3&gt;

&lt;p&gt;This is an overview of what I’ve been upto for the past 2 weeks. Doesn’t go into much technical details and the actual code but just walks through the general idea.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Convolution"&gt;Convolution&lt;/a&gt;  is a fundamental operation in various domains, such as image processing, signal processing, and deep learning. It is an important module in Gnuastro and is also used as a subroutine in other modules.&lt;/p&gt;

&lt;p&gt;Convolutional operations can be broken down into smaller tasks, such as applying the kernel to different portions of the input data. By utilizing multiple threads, each thread can independently process a subset of the input, reducing the overall execution time. This parallelization technique is particularly effective when dealing with large input tensors or performing multiple convolutions simultaneously.&lt;/p&gt;

&lt;p&gt;While traditional CPUs (Central Processing Units) excel at performing a wide range of tasks, they are not specifically designed for heavy parallel computations like convolutions. On the other hand, GPUs (Graphics Processing Units) are highly optimized for parallel processing, making them ideal for accelerating convolutional operations.&lt;/p&gt;

&lt;h3 id="gpus-vs-cpus-architecture"&gt;GPUs vs CPUs Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Architecture difference" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/architecture.png"&gt;&lt;/p&gt;

&lt;h4 id="cores-and-parallelism-"&gt;Cores and Parallelism :&lt;/h4&gt;
&lt;p&gt;CPUs have fewer, more powerful cores optimized for sequential processing, while GPUs have thousands of smaller cores designed for parallel processing. This parallelism allows GPUs to perform computations on multiple data elements simultaneously, leading to significant speedup in parallelizable tasks like graphics rendering and deep learning.&lt;/p&gt;

&lt;h4 id="memory-hierarchy-"&gt;Memory Hierarchy :&lt;/h4&gt;
&lt;p&gt;CPUs typically have larger caches and more advanced memory management units (MMUs), focusing on low-latency operations and complex branch prediction. GPUs, prioritize high memory bandwidth and utilize smaller caches to efficiently handle large amounts of data simultaneously, crucial for tasks like image processing and scientific simulations.&lt;/p&gt;

&lt;h4 id="emphasis-"&gt;Emphasis :&lt;/h4&gt;
&lt;p&gt;CPUs are designed with an emphasis on executing single threads - very fast. GPUs are designed with an emphasis on executing on executing multiple threads.&lt;/p&gt;

&lt;h3 id="programming-model"&gt;Programming Model&lt;/h3&gt;
&lt;p&gt;For Programming GPUs, several frameworks (high level APIs) are available&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CUDA - developed by NVIDIA for its GPUs.&lt;/li&gt;
&lt;li&gt;OpenCL - Open Source, Cross Platform parallel programming standard for diverse accelerators.&lt;/li&gt;
&lt;li&gt;HIP - developed by AMD, portable.&lt;/li&gt;
&lt;li&gt;and many more….&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="cuda"&gt;CUDA&lt;/h3&gt;

&lt;h4 id="the-cuda-platform-consists-of-a-programming-language-a-compiler-and-a-runtime-library"&gt;The CUDA platform consists of a programming language, a compiler, and a runtime library.&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code class="language-plaintext highlighter-rouge"&gt;Programming Language&lt;/code&gt; - Based on C, has extensions to write code for GPU.&lt;/li&gt;
&lt;li&gt;&lt;code class="language-plaintext highlighter-rouge"&gt;Compiler&lt;/code&gt; - Based on clang, offloads host code to system compiler and translates device code into binary code that can be executed on the GPU.&lt;/li&gt;
&lt;li&gt;&lt;code class="language-plaintext highlighter-rouge"&gt;Runtime Library&lt;/code&gt; - Provides the necessary functions and tools to manage the execution of the code on the GPU (interacts with the driver).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note : When we have multiple devices(GPUs, FPGAs, etc) on a single system, which can execute tasks apart from the main CPU, they’re generally referred to as &lt;code class="language-plaintext highlighter-rouge"&gt;device&lt;/code&gt; whereas the main CPU is referred to as &lt;code class="language-plaintext highlighter-rouge"&gt;host&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id="cuda-programs"&gt;CUDA Programs&lt;/h3&gt;

&lt;p&gt;CUDA programs consists of normal host code along with some &lt;code class="language-plaintext highlighter-rouge"&gt;kernels&lt;/code&gt;.
Kernels are like other functions, but when you call a kernel, they’re executed N times parallely by N different CUDA threads, as opposed to only once like normal functions. They’re defined using the &lt;code class="language-plaintext highlighter-rouge"&gt;__global__&lt;/code&gt; keyword.&lt;/p&gt;

&lt;p&gt;Eg :
&lt;img alt="kernel example" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/kernel.png"&gt;&lt;/p&gt;

&lt;p&gt;Normally, we put the above piece of code inside a loop, so all elements are covered.&lt;/p&gt;

&lt;p&gt;With GPUs, there’s no need for loops - for N elements, we launch N threads each of which add 1 element at the same time!&lt;/p&gt;

&lt;h3 id="cuda-execution-configuration"&gt;CUDA Execution Configuration&lt;/h3&gt;

&lt;p&gt;Can we launch an arbitrary large number of threads?
Technically No&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The maximum allowed threads depend on your GPUs compute capability.&lt;/li&gt;
&lt;li&gt;But generally it’s so large, it always covers all your elements&lt;/li&gt;
&lt;li&gt;For Compute Capability &amp;gt; 3.0
&lt;ul&gt;
&lt;li&gt;Max Number of threads : (2^31)&lt;em&gt;(2^16)&lt;/em&gt;(2^16)&lt;em&gt;(2&lt;/em&gt;10) = 2^42!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="threads-and-blocks-"&gt;Threads and Blocks :&lt;/h4&gt;

&lt;p&gt;&lt;img alt="Threads and Blocks" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/config.png"&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All threads are organized into groups called - Block.&lt;/li&gt;
&lt;li&gt;All blocks are organized into groups called - Grid.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Blocks and Grids could be a 1D, 2D or 3D structures.&lt;/p&gt;

&lt;p&gt;When calling a GPU kernel, we specify the structure of each block, number of blocks, and number of threads/block - This is called the Execution Configuration.&lt;/p&gt;

&lt;p&gt;Example :
&lt;img alt="Launching a kernel example" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/launch-kernel.png"&gt;&lt;/p&gt;

&lt;p&gt;The above code Launches
32&lt;em&gt;32&lt;/em&gt;1 = 1024 blocks
Each having 16&lt;em&gt;16 = 256 threads
Total no. of threads = 1024&lt;/em&gt;256.&lt;/p&gt;

&lt;h3 id="cuda-memory-hierarchy"&gt;CUDA Memory Hierarchy&lt;/h3&gt;

&lt;p&gt;&lt;img alt="Memory Hierarchy" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/memory.png"&gt;
CUDA threads may access data from multiple memory spaces during their execution as illustrated above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Local memory for each thread.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shared memory b/w all threads of same block.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Global memory b/w all blocks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="cuda-hardware-abstraction"&gt;CUDA Hardware abstraction&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Hardware Abstraction" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/hardware.png"&gt;&lt;/p&gt;

&lt;p&gt;The entire GPU is divided into several Streaming MultiProcessors (SMs). They have different architecture than a typical CPU core. Each SM has several CUDA cores, which are the actual processing units.&lt;/p&gt;

&lt;p&gt;It is designed with SIMT/SIMD philosophy, which allow execution of multiple threads concurrently on them. One Block is executed at a time on a single SM.&lt;/p&gt;

&lt;h3 id="cuda-developing-workflow"&gt;CUDA Developing Workflow&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Workflow" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/workflow.png"&gt;&lt;/p&gt;

&lt;h3 id="results-of-convolution-on-gpu-for-gnuastro"&gt;Results of Convolution on GPU for Gnuastro&lt;/h3&gt;

&lt;p&gt;All tests were performed on a system with the following specifications:&lt;/p&gt;

&lt;p&gt;CPU :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Intel(R) Core(TM) i5-9300HF CPU @ 2.40GHz&lt;/li&gt;
&lt;li&gt;Thread(s) per core:  2&lt;/li&gt;
&lt;li&gt;Core(s) per socket:  4&lt;/li&gt;
&lt;li&gt;Socket(s):           1&lt;/li&gt;
&lt;li&gt;CPU max MHz:         4100.0000&lt;/li&gt;
&lt;li&gt;CPU min MHz:         800.0000&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPU :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NVIDIA GeForce GTX 1650&lt;/li&gt;
&lt;li&gt;Turing Architecture&lt;/li&gt;
&lt;li&gt;Driver Version:      535.54.03&lt;/li&gt;
&lt;li&gt;CUDA Version:        12.2&lt;/li&gt;
&lt;li&gt;VRAM :               4GB&lt;/li&gt;
&lt;li&gt;Compute Capability : 7.5&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The input image was a 10k x 20k FITS file with 32-bit floating point values. The kernel was a 3x3 matrix with 32-bit floating point values.&lt;/p&gt;

&lt;h4 id="cpu-multi-threaded"&gt;CPU Multi-threaded&lt;/h4&gt;

&lt;p&gt;&lt;img alt="CPU" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/cpu-result.png"&gt;&lt;/p&gt;

&lt;h4 id="gpu"&gt;GPU&lt;/h4&gt;

&lt;p&gt;&lt;img alt="GPU" src="https://labeeb-7z.github.io/Blogs/img/posts/gpus/gpu-result.png"&gt;&lt;/p&gt;

&lt;p&gt;The overall speedups seems to only be 6X but this also counts the time taken to transfer the data from CPU to GPU and back. If we only consider the time taken to perform the convolution, the speedup is around ~700X!.&lt;/p&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2023/07/20230704_0000_labeeb-7z/</guid><pubDate>Mon, 03 Jul 2023 23:00:00 GMT</pubDate></item><item><title>Creating a new Data Structure for pyGnuastro</title><link>http://openastronomy.org/Universe_OA/posts/2023/06/20230620_0000_labeeb-7z/</link><dc:creator>Labib Asari</dc:creator><description>&lt;h3 id="background"&gt;Background&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://www.gnu.org/savannah-checkouts/gnu/gnuastro/gnuastro.html"&gt;GnuAstro&lt;/a&gt; is a powerful and comprehensive library designed to handle various data formats(FITS/TIFF/TXT and more) and perform a wide range of operations, all while maintaining consistency across its entire codebase.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;This is done by representing all the data (acquired via input or created internally), regardless of its type, in a single data structure which encompasses the core data as well as metadata. This greatly assists in mainting uniformity.
Internally all the data is represented in the form of a C struct : &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt;
The following image describes how it keeps the core data as well as metadata :
&lt;img alt="Code-Block1" src="https://labeeb-7z.github.io/Blogs/img/posts/creating-data-structure/gal_data_t.png"&gt;&lt;/p&gt;

&lt;p&gt;Explaining each attribute of this structure will require a seperate post of itself :). Instead I’ll focus on the main topic here : Since Im creating a python package for Gnuastro, and the &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; is at the heart of this library, How do I represent this complex type in Python?!&lt;/p&gt;

&lt;p&gt;Normally we use Classes to define new and complex data types in Python, but hey.. I’m wrapping a C library in Python using the Python-C API. This means I write my wrappers in C!&lt;/p&gt;

&lt;p&gt;So the question comes down to how do I create a new type in Python using C language?&lt;/p&gt;

&lt;h3 id="creating-new-data-types-in-python-without-classes-and-objects"&gt;Creating New Data Types in Python Without Classes and Objects&lt;/h3&gt;

&lt;p&gt;Before I continue, I’ve to appreciate &lt;a href="https://numpy.org/"&gt;Numpy&lt;/a&gt; for the incredible peice of software it is, the more I understand it, the more it amazes me.&lt;/p&gt;

&lt;p&gt;C is not an Object Oriented Programming Language, but Python is.&lt;/p&gt;

&lt;p&gt;In case you didn’t know the most common implementation of Python (the one you most probably have) is written in C! It’s called CPython.&lt;/p&gt;

&lt;p&gt;This raises an obvious question, how does Python implement its whole OOP paradigm in C?&lt;/p&gt;

&lt;p&gt;This question also answers our question of how to represent &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; in Python, because essentially they’re looking for the same thing.&lt;/p&gt;

&lt;p&gt;&lt;code class="language-plaintext highlighter-rouge"&gt;PyObject&lt;/code&gt; is the answer! To the Python interpreter(written in C) all the data types(built in as well as user defined) are of this type!&lt;/p&gt;

&lt;p&gt;and what is this &lt;code class="language-plaintext highlighter-rouge"&gt;PyObject&lt;/code&gt;? Its a simple struct in C.&lt;/p&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2023/06/20230620_0000_labeeb-7z/</guid><pubDate>Mon, 19 Jun 2023 23:00:00 GMT</pubDate></item><item><title>GSoC - its finally here</title><link>http://openastronomy.org/Universe_OA/posts/2023/06/20230605_0000_labeeb-7z/</link><dc:creator>Labib Asari</dc:creator><description>&lt;h3 id="what-is-open-source-and-gsoc"&gt;What is Open-Source and Gsoc?&lt;/h3&gt;
&lt;p&gt;Open source software is software with source code that anyone can inspect, modify, and enhance. There are many institutions and individuals who write open software, mainly for research or free deployment purposes. Mostly these softwares, have only a few maintainers, and multiple people, writing and debugging the code, helps a lot. This is where Google Summer of Code &lt;code class="language-plaintext highlighter-rouge"&gt;GSOC&lt;/code&gt; comes into the picture. It is a global, online program focused on bringing new contributors into open source software development. Many organisations float projects for the developers to take over the summer and Google mediates in the process, while also paying the contributors for their work over the summer.&lt;/p&gt;

&lt;!-- TEASER_END --&gt;
&lt;h3 id="what-is-my-project-about"&gt;What is my project about?&lt;/h3&gt;

&lt;p&gt;It has 2 main components :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a Python Library for Gnuastro
&lt;ul&gt;
&lt;li&gt;Design an error handling mechanism for Gnuastro&lt;/li&gt;
&lt;li&gt;Design corresponding data structures of Gnuastro in Python&lt;/li&gt;
&lt;li&gt;Write wrapper functions to be used in python&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add CUDA support in Gnuastro
&lt;ul&gt;
&lt;li&gt;Integrate CUDA with Gnuastro’s build system&lt;/li&gt;
&lt;li&gt;Write GPU kernels for compute heavy and parallelizable operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="what-have-i-completed-till-now"&gt;What have I completed till now?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;On the Python Library Part :
&lt;ul&gt;
&lt;li&gt;Gnuastro now has an error handling mechanism!&lt;/li&gt;
&lt;li&gt;Added error handling in Python package for the 2 existing modules.&lt;/li&gt;
&lt;li&gt;Defined error types for each corresponding error type in C library.&lt;/li&gt;
&lt;li&gt;Implemented Python wrappers for 2 of the C library modules&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;On the CUDA support part :
&lt;ul&gt;
&lt;li&gt;Gnuastro can now build with cuda! this means it already supports GPU computations.&lt;/li&gt;
&lt;li&gt;Added docs for installing, configuring, and testing CUDA&lt;/li&gt;
&lt;li&gt;Added test CUDA kernels and demo programs to test them.&lt;/li&gt;
&lt;li&gt;Implementing CUDA kernel for Convolution operation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;##&lt;/p&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2023/06/20230605_0000_labeeb-7z/</guid><pubDate>Sun, 04 Jun 2023 23:00:00 GMT</pubDate></item><item><title>GSoC - Pre Community Bonding</title><link>http://openastronomy.org/Universe_OA/posts/2023/05/20230507_0000_labeeb-7z/</link><dc:creator>Labib Asari</dc:creator><description>&lt;h3 id="what-is-open-source-and-gsoc"&gt;What is Open-Source and Gsoc?&lt;/h3&gt;
&lt;p&gt;Open source software is software with source code that anyone can inspect, modify, and enhance. There are many institutions and individuals who write open software, mainly for research or free deployment purposes. Mostly these softwares, have only a few maintainers, and multiple people, writing and debugging the code, helps a lot. This is where Google Summer of Code &lt;code class="language-plaintext highlighter-rouge"&gt;GSOC&lt;/code&gt; comes into the picture. It is a global, online program focused on bringing new contributors into open source software development. Many organisations float projects for the developers to take over the summer and Google mediates in the process, while also paying the contributors for their work over the summer.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2023/05/20230507_0000_labeeb-7z/</guid><pubDate>Sat, 06 May 2023 23:00:00 GMT</pubDate></item></channel></rss>